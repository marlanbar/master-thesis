% \section{Integrando el dataset Físico-Químico y el Genómico}

En este capítulo unimos los dos conjuntos de variables para evaluar si la integración de ambos datasets representan una mejora frente a los resultados del modelo Genómico y Físico-Químico por separado. A este nuevo dataset lo denominamos dataset Integral. Las variantes usadas fueron nuevamente las encontradas en la tabla Humsavar. 

\section{Creación del dataset Integral}

El dataset Integral posee 68,508 variantes. Este número equivale a la cantidad de variantes del dataset Físico-Químico, y esto se debe a que conservamos todas sus variantes sumando variables del Dataset Genómico (ver figura \ref{fig:interseccion_integral}). Esto da un total de 64 variables, que son las variables sumadas de los datasets Genómico (14), Físico-Químico (49) y la variable de respuesta.  

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{documents/latex/figures/3/integral/interseccion_integral.pdf}
    \caption{Intersección entre los datasets Genómico y Físico-Químico.}
    \label{fig:interseccion_integral}
\end{figure}

Del total de las variantes, 39,653 (58\%) son benignas y 28,855 (42\%) son patogénicas. Con respecto a los nulos, mantenemos la misma cobertura de las variables físico-químicas del dataset homónimo (aproximadamente 35\% para las variables categóricas y una cobertura mayor del 60\% para las continuas), mientras que para las variables genómicas tenemos alrededor de un 20\% de variantes que no poseen cobertura al no tener un identificador rsID.

\section{Generación del modelo}

Para este modelo preliminar utilizamos nuevamente el Pipeline Tree, repitiendo las mismas fases de imputación y entrenamiento usadas previamente en los modelos Genómico y Físico-Químico. Es decir, que en la fase de imputación usamos la mediana para los valores nulos de las variables continuas y el valor más frecuente en las variables categóricas. El entrenamiento consistió en una búsqueda de hiperparámetros óptimos usando \textit{GridSearch} en un diccionario de posibles parámetros (ver sección 7.3 del apéndice), evaluados en una triple validación cruzada basándonos en el AUC como métrica a optimizar. El dataset de entrenamiento posee 45,900 variantes, aproximadamente dos tercios del dataset completo. Una vez entrenado el modelo usando este \textit{pipeline}, se evaluaron las variantes del dataset de testeo, es decir, el tercio restante del dataset completo. Posteriormente utilizamos un método de boosting, Xtreme Gradient Boosting (XGBoost), nuevamente usando el Pipeline Tree. Los hiperparámetros de este método fueron elegidos usando una búsqueda randomizada (Randomized Grid Search). Este método de optimización fue elegido dado que las combinaciones que se evalúan en el método Grid Search son demasiadas (ver sección 7.3 del apéndice para diccionario completo de hiperparámetros). La búsqueda randomizada evalúa las mismas alternativas pero eligiendo combinaciones de forma aleatoria, sin probar todas las combinaciones. También es posible evaluar valores tomados de acuerdo a una distribución específica. Decidimos no hacer este análisis en nuestro trabajo.

\section{Resultados del modelo Integral}

El modelo Random Forest obtuvo un AUC de 0.88  (ver figura \ref{fig:auc_integral}). Esto representa una mejora con respecto al modelo Genómico (0.85) y al modelo Físico-Químico (0.72). Los hiperparámetros escogidos fueron: profundidad del árbol 7, 20\% de las variables en consideración al realizar el mejor corte y 100 árboles.
Si comparamos las métricas obtenidas en el modelo Genómico con las de este modelo (ver tabla \ref{tab:metrics_integral_rf}), se puede apreciar un nuevo salto en el \textit{recall} de las variables patogénicas, que pasa de un 0.64 en el modelo Genómico al 0.80, lo que representa un 25\% de mejora. También mejora la precisión con respecto a la detección de esta clase, que pasa de 0.69 a 0.76. La única métrica que decae es el \textit{recall} de la clase Benigna, que pasa de 0.87 a 0.82, lo que representa una leve caída del \textit{f1-score}. 
Finalmente, el modelo XGBoost superó la performance de Random Forest alcanzando un AUC de 0.90. Utilizando el test de Mann-Whitney para comparar los dos AUCs obtuvimos un p-valor igual a $2\mathrm{e}{-16}$, lo que indica que este resultado no se debe al azar. Sus hiperparámetros escogidos fueron:

\begin{itemize}
    \item min\_child\_weight: 5
    \item gamma: 5
    \item subsample: 0.8
    \item colsample\_bytree: 0.8
    \item max\_depth: 5
\end{itemize}

En la tabla \ref{tab:metrics_model_integral} comparamos la Precisión, el Recall y el AUC y los tiempos de entrenamiento (usando las variantes del set de entrenamiento completo) y el tiempo de predicción de todas las variables del set de evaluación. Las métricas están basadas en las variantes patogénicas como variantes positivas. En las tablas \ref{tab:metrics_integral_rf} y \ref{tab:metrics_integral_xgb} evaluamos la posibilidad de tomar variables benignas como positivas en cada uno de los modelos. El modelo XGB supera al modelo RF en casi todas las métricas exceptuando al tiempo de entrenamiento, que incluso usando el método de busca de hiperparámetros randomizado resultó ser mucho más lento que el modelo RF, si bien es posible reducir aún más el espacio de búsqueda. Esto se debe mayormente a que a diferencia de RF que genera estimadores de forma paralela, XGB es iterativo, lo que ralentiza el proceso. 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
Modelo & Precisión & Recall & AUC & F1-score & $t_{fit}$ & $t_{pred}$ \\ \hline
RF & 0.76 & 0.80 & 0.88 & 0.78 & 2m 2 s & 0.3 s \\ \hline
XGB & 0.78 & 0.82 & 0.90 & 0.80 & 12m 47s & 1.14 s \\ \hline
\end{tabular}
\caption{Comparación de métricas de modelos usando el dataset Integral. Las variables $t_{fit}$ y $t_{pred}$ corresponden al tiempo de entrenamiento y de predicción de todas las variantes}
\label{tab:metrics_model_integral}

\end{table}


\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Clase        & Precisión & Recall & F1-score \\ \hline
Benignas     & 0.85      & 0.82   & 0.83     \\ \hline
Patogénicas  & 0.76      & 0.80   & 0.78     \\ \hline
Promedio     & 0.81      & 0.81   & 0.81     \\ \hline
\end{tabular}
\caption{Reporte de métricas del modelo Random Forest usando el dataset Integral.}
\label{tab:metrics_integral_rf}
\end{table}


\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Clase        & Precisión & Recall & F1-score \\ \hline
Benignas     & 0.86      & 0.83   & 0.84     \\ \hline
Patogénicas  & 0.78      & 0.82   & 0.80     \\ \hline
Promedio     & 0.83      & 0.82   & 0.82     \\ \hline
\end{tabular}
\caption{Reporte de métricas del modelo XGB usando el dataset Integral.}
\label{tab:metrics_integral_xgb}
\end{table}

\section{Importancia de las variables}

Al combinar los dos datasets Genómico y Físico-Químico volvimos a evaluar la importancia de las variables en los modelos, dado que las nuevas interacciones entre ellas pueden haber modificado los resultados anteriores. 

Si analizamos solamente la importancia usando el método de \texttt{scikit-learn} para el modelo RF (ver figura \ref{fig:importances_integral_rf}), nuevamente encontramos en primer lugar a las variables de conservación genómicas (PHYLOP46WAY y PHASTCONS46WAY), resultado esperable dado su nivel de importancia en el dataset Genómico y su nivel de AUC conseguido. También encontramos en un segundo escalón a las variables de conservación a nivel de exones, y a una variable que considera el número de SNPs en el exón donde ocurre la mutación. Luego encontramos al grupo de matrices de sustitución de aminoácidos (EX, GRANTHAM, BLOSUM y PAM250), que también aparecieron en los primeros lugares en el modelo Físico-Químico. Por último encontramos una variable relativa a la clase funcional a nivel genómico (MISSENSE) y otra relativa al cambio de polaridad del aminoácido donde ocurre la variante (POLARITY), por lo que encontramos en nuestro ranking una lista de variables transversal a los dos datasets usados. 


En las figuras \ref{fig:importance_cluster_integral_rf} y \ref{fig:importance_cluster_integral_xgb} unimos las variables de alta correlación en clusters y comparamos su impacto en la precisión del modelo usando la herramienta \texttt{rfpimp}. Notamos que la importancia de CONS se ve disminuida en el modelo XGB con respecto al modelo RF.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{documents/latex/figures/3/integral/integral_importance_cluster.pdf}
    \caption{Importancia de variables altamente correlacionadas (basados en correlación de Spearman) usando permutación. Resultados del modelo usando Random Forest.}
    \label{fig:importance_cluster_integral_rf}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{documents/latex/figures/3/integral/integral_importance_cluster_xgb.pdf}
    \caption{Importancia de variables altamente correlacionadas (basados en correlación de Spearman) usando permutación. Resultados del modelo usando XGB.}
    \label{fig:importance_cluster_integral_xgb}
\end{figure}


% \begin{figure}[H]
% \begin{subfigure}[b]{0.5\textwidth}
%     \includegraphics[width=\textwidth]{documents/latex/figures/3/integral/integral_importance_cluster.pdf}
%     \caption{Importancia de variables clusterizadas (basados en correlación de Spearman) usando permutación. Resultados del modelo usando Random Forest.}
%     \label{fig:importance_cluster_integral_rf}
% \end{subfigure}
% \hfill
% \hfill
% \begin{subfigure}[b]{0.5\textwidth}
%     \includegraphics[width=\textwidth]{documents/latex/figures/3/integral/integral_importance_cluster_xgb.pdf}
%     \caption{Importancia de variables clusterizadas (basados en correlación de Spearman) usando permutación. Resultados del modelo usando XGB.}
%     \label{fig:importance_cluster_integral_xgb}
% \end{subfigure}

% \end{figure}





\newpage

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{documents/latex/figures/3/integral/auc_integral.pdf}
    \caption{Comparación de curvas AUC entre algoritmos Random Forest y XGBoost. La línea punteada corresponde a un predictor Random.}
    \label{fig:auc_integral}
\end{subfigure}
\hfill
\hfill
\begin{subfigure}[b]{0.7\textwidth}
    \centering
    \includegraphics[width=\textwidth]{documents/latex/figures/3/integral/importances_integral.pdf}
    \caption{Los 10 atributos más importantes del dataset Integral aportados por el algoritmo Random Forest.}
    \label{fig:importances_integral_rf}
\end{subfigure}

\caption{Curva AUC y atributos más importantes de los modelos RF y XGBoost usando el dataset Integral.}

\end{figure}
